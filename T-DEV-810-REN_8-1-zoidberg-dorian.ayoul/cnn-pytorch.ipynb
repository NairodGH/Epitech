{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Transformation\n",
    "\n",
    "The provided code snippet demonstrates how we load image data using PyTorch's `ImageFolder` and `DataLoader` modules, while also applying custom preprocessing transformations.\n",
    "\n",
    "1. **Custom Transformations:**\n",
    "   - Two custom transformation classes are defined:\n",
    "     - **Denoise:** Applies Gaussian blur to the image using `ImageFilter.GaussianBlur`.\n",
    "     - **HistogramEqualization:** Performs histogram equalization on the image. If the image is RGB, it converts it to grayscale (`cv2.COLOR_RGB2GRAY`) before equalizing the histogram.\n",
    "\n",
    "2. **Transformation Pipeline:**\n",
    "   - A `transforms.Compose` object chains together several transformations:\n",
    "     - `HistogramEqualization()`: Custom transformation to enhance image contrast.\n",
    "     - `Denoise()`: Custom transformation to reduce noise using Gaussian blur.\n",
    "     - `transforms.Resize((224, 224))`: Resizes the image to a fixed size of 224x224 pixels.\n",
    "     - `transforms.ToTensor()`: Converts the PIL Image to a PyTorch tensor.\n",
    "     - (Commented out) `transforms.Normalize()`: Normalization step which is currently disabled.\n",
    "\n",
    "3. **Dataset Loading:**\n",
    "   - `ImageFolder` is used to create a dataset from images located at the specified `path`, applying the defined transformations.\n",
    "   - `DataLoader` is then used to create batches of the dataset for training or evaluation.\n",
    "\n",
    "4. **Subset (Optional):**\n",
    "   - The code includes a commented-out line (`Subset`) which can be used to create a subset of the dataset if needed.\n",
    "\n",
    "5. **Batch Size and Subset Size:**\n",
    "   - Parameters `batch_size` and `subset_size` control the size of batches and the subset of the dataset to load, respectively.\n",
    "\n",
    "This setup prepares the image data for further processing or training using PyTorch, with the flexibility to add or modify transformations as needed.\n",
    "\n",
    "We utilized this code to customize our images and aimed to achieve optimal results. \n",
    "To achieve this, we experimented with the `transforms.Compose` function by adding and removing transformations within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import ImageFilter\n",
    "\n",
    "class Denoise:\n",
    "    def __call__(self, img):\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=1))\n",
    "        return img\n",
    "\n",
    "class HistogramEqualization:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.equalizeHist(img)\n",
    "        return F.to_pil_image(img)\n",
    "\n",
    "\n",
    "def load_data(path, subset_size=624, batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        HistogramEqualization(),\n",
    "        Denoise(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "\n",
    "    dataset_folder = ImageFolder(root=path, transform=transform)\n",
    "    # subset = Subset(dataset_folder, range(subset_size))\n",
    "    dataset = DataLoader(dataset_folder, batch_size=batch_size)\n",
    "\n",
    "    # print('Dataset\\t', 'Train\\t', dataset_folder.classes[0], '', dataset_folder.classes[1])\n",
    "    # print('Total:\\t', len(dataset_folder), '\\t', dataset_folder.targets.count(0), '\\t', dataset_folder.targets.count(1))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our method: Visualizing and Improving Data Transformations\n",
    "\n",
    "We utilized a methodical approach to enhance image data transformations:\n",
    "\n",
    "1. **Visualization and Adjustment:**  \n",
    "   We converted images to PNG format to visually assess the effects of transformations like denoising and histogram equalization.\n",
    "\n",
    "2. **Human-Centric Evaluation:**  \n",
    "   This method allowed for straightforward evaluation of transformation impacts, guiding iterative adjustments to our `transforms.Compose` pipeline.\n",
    "\n",
    "3. **Iterative Refinement:**  \n",
    "   By iteratively refining transformations based on visual feedback, we optimized our image preprocessing for better performance in machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_transformed_images(dataset, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for i, (image, label) in enumerate(tqdm(dataset)):\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        image.save(os.path.join(path, f'imagey_{i}_label_{label}.png'))\n",
    "\n",
    "data_path = 'datasets/train'\n",
    "\n",
    "dataset = load_data(path=data_path, batch_size=64)\n",
    "\n",
    "save_transformed_images(dataset.dataset, 'datasets/train_transformed/NORMAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of to images before and after applying transformations:\n",
    "\n",
    "### **Original Image:**\n",
    "  ![Original Image](resources/jupiter/normal.png)\n",
    "\n",
    "### **Transformed Image:**\n",
    "  ![Transformed Image](resources/jupiter/transformed.png)\n",
    "\n",
    "By following this method, we were able to fine-tune our data transformations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network\n",
    "\n",
    "Here is the code of the network we used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 28 * 28)  # Flatten the output of conv3 layer\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method Summary\n",
    "\n",
    "1. **Training Loop:**\n",
    "   - Trains a CNN model on the provided dataset for a specified number of epochs.\n",
    "   - Uses CrossEntropyLoss and Adam optimizer.\n",
    "   - Tracks and prints training loss and accuracy.\n",
    "\n",
    "2. **Model Saving:**\n",
    "   - Saves the trained model state to a file specified by the user or defaults to 'model.pth'.\n",
    "\n",
    "3. **Device Setup:**\n",
    "   - Utilizes GPU if available, otherwise falls back to CPU.\n",
    "\n",
    "4. **Execution:**\n",
    "   - Loads the dataset.\n",
    "   - Initializes the model.\n",
    "   - Trains the model.\n",
    "   - Saves the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "def train_model(dataset, model, num_epochs=20):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    accuracy_total_train = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(total=len(dataset), desc='Epoch {}/{}'.format(epoch+1, num_epochs), position=0, leave=True)\n",
    "\n",
    "        for inputs, labels in dataset:\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            accuracy_total_train.append(torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        progress_bar.close()\n",
    "        print('Loss: {:.4f}'.format(epoch_loss), 'Accuracy: {:.4f}'.format(sum(accuracy_total_train) / len(accuracy_total_train)))\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n",
    "model_name = 'model.pth'\n",
    "if len(sys.argv) > 1:\n",
    "    model_name = sys.argv[1]\n",
    "\n",
    "# Load datasets\n",
    "dataset = load_data('datasets/train_transformed', subset_size=5216, batch_size=128)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device, '\\n')\n",
    "\n",
    "# Create an instance of the model\n",
    "num_classes = len(dataset.dataset.classes)\n",
    "model = CNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# Training loop\n",
    "model = train_model(dataset, model, num_epochs=5)\n",
    "\n",
    "print('Saving the model...')\n",
    "torch.save(model.state_dict(), model_name)\n",
    "print('Model saved as', model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Method Summary\n",
    "\n",
    "1. **Testing Loop:**\n",
    "   - Evaluates the trained model on the test dataset.\n",
    "   - Computes and prints the test accuracy.\n",
    "\n",
    "2. **Device Setup:**\n",
    "    - Utilizes GPU if available, otherwise falls back to CPU.\n",
    "\n",
    "3. **Execution:**\n",
    "    - Loads the test dataset.\n",
    "    - Initializes the model.\n",
    "    - Loads the weights of the trained model.\n",
    "    - Tests the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "model_name = 'model.pth'\n",
    "if len(sys.argv) > 1:\n",
    "    model_name = sys.argv[1]\n",
    "\n",
    "dataset = load_data(path='datasets/test', batch_size=64)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device, '\\n')\n",
    "\n",
    "# Create an instance of the model\n",
    "num_classes = len(dataset.dataset.classes)\n",
    "model = CNN(num_classes=num_classes).to(device)\n",
    "\n",
    "print('Loading the model', model_name + '...')\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "progress_bar = tqdm(total=len(dataset), desc='Testing', position=0, leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataset:\n",
    "        logits = model.forward(inputs)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "The model evaluation process is based on testing the trained model on unseen data to make sure it generalizes well. The test accuracy provides insights into the model's performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "To conclude, the use of a CNN and proper data transformations can significantly improve the performance of patern recognition. To improve the model's performance, we experimented with various data transformations, but finetuning the network architecture could be investigated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T-DEV-810-REN_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
